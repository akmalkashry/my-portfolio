{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Form tokenization and Filter stop words & punctuation**"
      ],
      "metadata": {
        "id": "OK6Bf_IxUDAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tDemonstrate sentence segmentation and report the output"
      ],
      "metadata": {
        "id": "FF81NYgxUHaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vbMSwlAIoES",
        "outputId": "03b62b87-3aa1-4340-bc7d-ce36a9705558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: Textual information in the world can be broadly categorized into two main types: facts and opinions.\n",
            "Sentence 2: Facts are objective expressions about entities, events, and their properties.\n",
            "Sentence 3: Opinions are usually subjective expressions that describe people’s sentiments, appraisals, or feelings toward entities, events, and their properties.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Read file\n",
        "with open('/Individual Assingment/Data_1.txt', 'r') as file:\n",
        "    data_1 = file.read()\n",
        "\n",
        "# Sentence segmentation\n",
        "sentences = nltk.sent_tokenize(data_1)\n",
        "\n",
        "# Result\n",
        "for i, sentence in enumerate(sentences, start=1):\n",
        "    print(f\"Sentence {i}: {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tDemonstrate word tokenisation using the split function, Regular Expression and NLTK packages separately and report the output"
      ],
      "metadata": {
        "id": "lvnayZ3-UMJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize using the 'Split Function'\n",
        "tokens = data_1.split()\n",
        "\n",
        "# Report the tokens\n",
        "for token in tokens:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2bvSDfuKdaX",
        "outputId": "6cea9025-9e75-4ee8-89b4-23d8f3ab7768"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textual\n",
            "information\n",
            "in\n",
            "the\n",
            "world\n",
            "can\n",
            "be\n",
            "broadly\n",
            "categorized\n",
            "into\n",
            "two\n",
            "main\n",
            "types:\n",
            "facts\n",
            "and\n",
            "opinions.\n",
            "Facts\n",
            "are\n",
            "objective\n",
            "expressions\n",
            "about\n",
            "entities,\n",
            "events,\n",
            "and\n",
            "their\n",
            "properties.\n",
            "Opinions\n",
            "are\n",
            "usually\n",
            "subjective\n",
            "expressions\n",
            "that\n",
            "describe\n",
            "people’s\n",
            "sentiments,\n",
            "appraisals,\n",
            "or\n",
            "feelings\n",
            "toward\n",
            "entities,\n",
            "events,\n",
            "and\n",
            "their\n",
            "properties.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Tokenize using 'Regular Expressions'\n",
        "tokens = re.findall(r'\\b\\w+\\b', data_1)\n",
        "\n",
        "# Report the tokens\n",
        "for token in tokens:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsJdekqQSjXC",
        "outputId": "d009dfdf-777d-47dd-9d32-f11c79aedd41"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textual\n",
            "information\n",
            "in\n",
            "the\n",
            "world\n",
            "can\n",
            "be\n",
            "broadly\n",
            "categorized\n",
            "into\n",
            "two\n",
            "main\n",
            "types\n",
            "facts\n",
            "and\n",
            "opinions\n",
            "Facts\n",
            "are\n",
            "objective\n",
            "expressions\n",
            "about\n",
            "entities\n",
            "events\n",
            "and\n",
            "their\n",
            "properties\n",
            "Opinions\n",
            "are\n",
            "usually\n",
            "subjective\n",
            "expressions\n",
            "that\n",
            "describe\n",
            "people\n",
            "s\n",
            "sentiments\n",
            "appraisals\n",
            "or\n",
            "feelings\n",
            "toward\n",
            "entities\n",
            "events\n",
            "and\n",
            "their\n",
            "properties\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize using 'NLTK'\n",
        "tokens = nltk.word_tokenize(data_1)\n",
        "\n",
        "# Report the tokens\n",
        "for token in tokens:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxfDFm_7SuGK",
        "outputId": "1a18510f-e615-4f3b-c9cd-308fb1cae75c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textual\n",
            "information\n",
            "in\n",
            "the\n",
            "world\n",
            "can\n",
            "be\n",
            "broadly\n",
            "categorized\n",
            "into\n",
            "two\n",
            "main\n",
            "types\n",
            ":\n",
            "facts\n",
            "and\n",
            "opinions\n",
            ".\n",
            "Facts\n",
            "are\n",
            "objective\n",
            "expressions\n",
            "about\n",
            "entities\n",
            ",\n",
            "events\n",
            ",\n",
            "and\n",
            "their\n",
            "properties\n",
            ".\n",
            "Opinions\n",
            "are\n",
            "usually\n",
            "subjective\n",
            "expressions\n",
            "that\n",
            "describe\n",
            "people\n",
            "’\n",
            "s\n",
            "sentiments\n",
            ",\n",
            "appraisals\n",
            ",\n",
            "or\n",
            "feelings\n",
            "toward\n",
            "entities\n",
            ",\n",
            "events\n",
            ",\n",
            "and\n",
            "their\n",
            "properties\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\tJustify the most suitable tokenisation operation for text analytics. Support your answer using obtained outputs"
      ],
      "metadata": {
        "id": "Pucu2Qp1UWpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize using the 'Split Function'\n",
        "tokens = data_1.split()\n",
        "\n",
        "# Report the tokens\n",
        "for token in tokens:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFrRogzyWQar",
        "outputId": "070dfed2-f657-4b5d-ef11-841dd15fd7f5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textual\n",
            "information\n",
            "in\n",
            "the\n",
            "world\n",
            "can\n",
            "be\n",
            "broadly\n",
            "categorized\n",
            "into\n",
            "two\n",
            "main\n",
            "types:\n",
            "facts\n",
            "and\n",
            "opinions.\n",
            "Facts\n",
            "are\n",
            "objective\n",
            "expressions\n",
            "about\n",
            "entities,\n",
            "events,\n",
            "and\n",
            "their\n",
            "properties.\n",
            "Opinions\n",
            "are\n",
            "usually\n",
            "subjective\n",
            "expressions\n",
            "that\n",
            "describe\n",
            "people’s\n",
            "sentiments,\n",
            "appraisals,\n",
            "or\n",
            "feelings\n",
            "toward\n",
            "entities,\n",
            "events,\n",
            "and\n",
            "their\n",
            "properties.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenize using 'Regular Expressions'\n",
        "tokens = re.findall(r'\\b\\w+\\b', data_1)\n",
        "\n",
        "# Report the tokens\n",
        "for token in tokens:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW45Hw0RWTQl",
        "outputId": "66eb87ed-eaa5-4c0e-f9f1-054e117a8925"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textual\n",
            "information\n",
            "in\n",
            "the\n",
            "world\n",
            "can\n",
            "be\n",
            "broadly\n",
            "categorized\n",
            "into\n",
            "two\n",
            "main\n",
            "types\n",
            "facts\n",
            "and\n",
            "opinions\n",
            "Facts\n",
            "are\n",
            "objective\n",
            "expressions\n",
            "about\n",
            "entities\n",
            "events\n",
            "and\n",
            "their\n",
            "properties\n",
            "Opinions\n",
            "are\n",
            "usually\n",
            "subjective\n",
            "expressions\n",
            "that\n",
            "describe\n",
            "people\n",
            "s\n",
            "sentiments\n",
            "appraisals\n",
            "or\n",
            "feelings\n",
            "toward\n",
            "entities\n",
            "events\n",
            "and\n",
            "their\n",
            "properties\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize using NLTK\n",
        "tokens = nltk.word_tokenize(data_1)\n",
        "\n",
        "# Report the tokens\n",
        "for token in tokens:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr0wFB7XS7iD",
        "outputId": "9e055cc5-bdb7-4343-96d8-62d2616f63ed"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textual\n",
            "information\n",
            "in\n",
            "the\n",
            "world\n",
            "can\n",
            "be\n",
            "broadly\n",
            "categorized\n",
            "into\n",
            "two\n",
            "main\n",
            "types\n",
            ":\n",
            "facts\n",
            "and\n",
            "opinions\n",
            ".\n",
            "Facts\n",
            "are\n",
            "objective\n",
            "expressions\n",
            "about\n",
            "entities\n",
            ",\n",
            "events\n",
            ",\n",
            "and\n",
            "their\n",
            "properties\n",
            ".\n",
            "Opinions\n",
            "are\n",
            "usually\n",
            "subjective\n",
            "expressions\n",
            "that\n",
            "describe\n",
            "people\n",
            "’\n",
            "s\n",
            "sentiments\n",
            ",\n",
            "appraisals\n",
            ",\n",
            "or\n",
            "feelings\n",
            "toward\n",
            "entities\n",
            ",\n",
            "events\n",
            ",\n",
            "and\n",
            "their\n",
            "properties\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tDemonstrate stop words and punctuations removal from the given text corpus and report the output suitably"
      ],
      "metadata": {
        "id": "cmYAGaidUjmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Read the text corpus from a file\n",
        "file_path_1 = '/Individual Assingment/Data_1.txt'  # Replace with the path to your text file\n",
        "with open(file_path_1, 'r') as file:\n",
        "    corpus = file.read()\n",
        "\n",
        "# Tokenize the corpus into sentences\n",
        "sentences = nltk.sent_tokenize(corpus)\n",
        "\n",
        "# Remove punctuation\n",
        "translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "sentences_without_punctuations = [sentence.translate(translator) for sentence in sentences]\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sentences_without_stopwords = [\n",
        "    \" \".join([word for word in sentence.lower().split() if word not in stop_words])\n",
        "    for sentence in sentences_without_punctuations\n",
        "]\n",
        "\n",
        "# Report the processed corpus\n",
        "for i, sentence in enumerate(sentences_without_stopwords, start=1):\n",
        "    print(f\"Sentence {i}: {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UFPCqxQUhnZ",
        "outputId": "ca4f6816-704a-4c51-e4b3-4e774dcadeeb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: textual information world broadly categorized two main types facts opinions\n",
            "Sentence 2: facts objective expressions entities events properties\n",
            "Sentence 3: opinions usually subjective expressions describe people’s sentiments appraisals feelings toward entities events properties\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\tReport the stop words found in the given text corpus"
      ],
      "metadata": {
        "id": "QOtqMtT6cS72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get the English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Find the stop words in the corpus\n",
        "stop_words_found = [word for word in corpus.lower().split() if word in stop_words]\n",
        "\n",
        "# Report the stop words\n",
        "print(\"Stop words found in the corpus:\")\n",
        "print(stop_words_found)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvctM0wYUit7",
        "outputId": "64384307-ae56-41ed-c440-febfa4fcd585"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words found in the corpus:\n",
            "['in', 'the', 'can', 'be', 'into', 'and', 'are', 'about', 'and', 'their', 'are', 'that', 'or', 'and', 'their']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Form word stemming**"
      ],
      "metadata": {
        "id": "GKwxMM5Ndvqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tDemonstrate word stemming using Regular Expression, Porter Stemmer and Lancaster Stemmer and report the output."
      ],
      "metadata": {
        "id": "7qvxKn20qwVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "\n",
        "# Read the text corpus from a file\n",
        "file_path_1 = '/Individual Assingment/Data_1.txt'  # Replace with the path to your text file\n",
        "with open(file_path_1, 'r') as file:\n",
        "    corpus = file.read()\n",
        "\n",
        "# Word stemming using Regular Expression\n",
        "stemmed_regex = [re.findall(r'\\b\\w+\\b', word.lower())[0] for word in corpus.split()]\n",
        "\n",
        "# Word stemming using Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "stemmed_porter = [porter.stem(word) for word in corpus.split()]\n",
        "\n",
        "# Word stemming using Lancaster Stemmer\n",
        "lancaster = LancasterStemmer()\n",
        "stemmed_lancaster = [lancaster.stem(word) for word in corpus.split()]\n",
        "\n",
        "# Report the stemmed words\n",
        "print(\"Stemmed words using Regular Expression:\")\n",
        "print(stemmed_regex)\n",
        "print(\"\\nStemmed words using Porter Stemmer:\")\n",
        "print(stemmed_porter)\n",
        "print(\"\\nStemmed words using Lancaster Stemmer:\")\n",
        "print(stemmed_lancaster)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1Bb7DUvdw6x",
        "outputId": "56e213bb-cc86-48f0-fb0e-7f9bf08984c7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words using Regular Expression:\n",
            "['textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', 'facts', 'and', 'opinions', 'facts', 'are', 'objective', 'expressions', 'about', 'entities', 'events', 'and', 'their', 'properties', 'opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', 'sentiments', 'appraisals', 'or', 'feelings', 'toward', 'entities', 'events', 'and', 'their', 'properties']\n",
            "\n",
            "Stemmed words using Porter Stemmer:\n",
            "['textual', 'inform', 'in', 'the', 'world', 'can', 'be', 'broadli', 'categor', 'into', 'two', 'main', 'types:', 'fact', 'and', 'opinions.', 'fact', 'are', 'object', 'express', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'opinion', 'are', 'usual', 'subject', 'express', 'that', 'describ', 'people’', 'sentiments,', 'appraisals,', 'or', 'feel', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.']\n",
            "\n",
            "Stemmed words using Lancaster Stemmer:\n",
            "['text', 'inform', 'in', 'the', 'world', 'can', 'be', 'broad', 'categ', 'into', 'two', 'main', 'types:', 'fact', 'and', 'opinions.', 'fact', 'ar', 'object', 'express', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'opin', 'ar', 'us', 'subject', 'express', 'that', 'describ', 'people’s', 'sentiments,', 'appraisals,', 'or', 'feel', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. Form Parts of Speech (POS) taggers & Syntactic Analysers**"
      ],
      "metadata": {
        "id": "HbnxABpov9x_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tDemonstrate POS tagging using NLTK POS tagger, textblob POS tagger and the Regular Expression tagger and report the output"
      ],
      "metadata": {
        "id": "XJuGEqZDwrem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Read text from file\n",
        "with open('/Individual Assingment/Data_2.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# POS tagging using NLTK POS tagger\n",
        "nltk_pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# POS tagging using TextBlob POS tagger\n",
        "textblob_pos_tags = TextBlob(text).tags\n",
        "\n",
        "# Define the regular expression patterns for POS tagging\n",
        "patterns = [\n",
        "    (r'\\b[Aa][Nn][Dd]\\b', 'CC'),  # Coordinating Conjunction\n",
        "    (r'\\b[Tt][Hh][Ee]\\b', 'DT'),  # Determiner\n",
        "    (r'\\b[Aa]\\b', 'DT'),  # Determiner\n",
        "    # Add more patterns for other POS tags as needed\n",
        "]\n",
        "\n",
        "# POS tagging using Regular Expression tagger\n",
        "regexp_tagger = nltk.RegexpTagger(patterns)\n",
        "regexp_pos_tags = regexp_tagger.tag(words)\n",
        "\n",
        "# Report the POS tags\n",
        "print(\"NLTK POS Tags:\")\n",
        "print(nltk_pos_tags)\n",
        "print(\"\\nTextBlob POS Tags:\")\n",
        "print(textblob_pos_tags)\n",
        "print(\"\\nRegular Expression POS Tags:\")\n",
        "print(regexp_pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfYK3OW7wBP5",
        "outputId": "582e7448-76ec-4196-d486-7fbfadd6df37"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK POS Tags:\n",
            "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB'), ('.', '.')]\n",
            "\n",
            "TextBlob POS Tags:\n",
            "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB')]\n",
            "\n",
            "Regular Expression POS Tags:\n",
            "[('The', 'DT'), ('big', None), ('black', None), ('dog', None), ('barked', None), ('at', None), ('the', 'DT'), ('white', None), ('cat', None), ('and', 'CC'), ('chased', None), ('away', None), ('.', None)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tDraw possible parse trees for the given sentence using suitable python codes and report the output along with the code. "
      ],
      "metadata": {
        "id": "v1yZTyJ01-HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import CFG\n",
        "from nltk.tree import Tree\n",
        "\n",
        "# Define the sentence\n",
        "sentence = \"The big black dog barked at the white cat and chased away.\"\n",
        "\n",
        "# Define a context-free grammar (CFG) for parsing\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "    S -> NP VP\n",
        "    NP -> DT JJ JJ NN | DT JJ NN | DT NN\n",
        "    VP -> VB PP | VB NP\n",
        "    PP -> IN NP\n",
        "    DT -> 'The'\n",
        "    JJ -> 'big' | 'black' | 'white'\n",
        "    NN -> 'dog' | 'cat'\n",
        "    VB -> 'barked' | 'chased'\n",
        "    IN -> 'at' | 'and'\n",
        "    V -> 'away'\n",
        "\"\"\")\n",
        "\n",
        "# Create a parser based on the CFG\n",
        "parser = nltk.ChartParser(grammar)\n",
        "\n",
        "# Parse the sentence and visualize the parse trees\n",
        "for tree in parser.parse(sentence.split()):\n",
        "    tree.pretty_print()\n"
      ],
      "metadata": {
        "id": "-TTQVncx2T0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Unsmoothed bigram language model\n",
        "def unsmoothed_bigram(sentence, corpus):\n",
        "    sentence_tokens = sentence.split()\n",
        "    sentence_prob = 1.0\n",
        "\n",
        "    for i in range(1, len(sentence_tokens)):\n",
        "        prev_word = sentence_tokens[i-1]\n",
        "        curr_word = sentence_tokens[i]\n",
        "        bigram_count = corpus.count(prev_word + ' ' + curr_word)\n",
        "        prev_word_count = corpus.count(prev_word)\n",
        "        bigram_prob = bigram_count / prev_word_count if prev_word_count > 0 else 0.0\n",
        "        sentence_prob *= bigram_prob\n",
        "\n",
        "    return sentence_prob\n",
        "\n",
        "# Smoothed bigram language model with add-one smoothing\n",
        "def smoothed_bigram(sentence, corpus):\n",
        "    sentence_tokens = sentence.split()\n",
        "    sentence_prob = 1.0\n",
        "    vocabulary_size = len(set(corpus.split()))\n",
        "\n",
        "    for i in range(1, len(sentence_tokens)):\n",
        "        prev_word = sentence_tokens[i-1]\n",
        "        curr_word = sentence_tokens[i]\n",
        "        bigram_count = corpus.count(prev_word + ' ' + curr_word)\n",
        "        prev_word_count = corpus.count(prev_word)\n",
        "        bigram_prob = (bigram_count + 1) / (prev_word_count + vocabulary_size)\n",
        "        sentence_prob *= bigram_prob\n",
        "\n",
        "    return sentence_prob\n",
        "\n",
        "# Training Corpus\n",
        "corpus = \"<s> He read a book </s> <s> I read a different book </s> <s> He read a book by Danielle </s>\"\n",
        "\n",
        "# Sentence to calculate probability\n",
        "sentence = \"<s> I read a book by Danielle </s>\"\n",
        "\n",
        "# Calculate sentence probability using unsmoothed bigram model\n",
        "unsmoothed_prob = unsmoothed_bigram(sentence, corpus)\n",
        "print(\"Unsmoothed Bigram Probability:\", unsmoothed_prob)\n",
        "\n",
        "# Calculate sentence probability using smoothed bigram model\n",
        "smoothed_prob = smoothed_bigram(sentence, corpus)\n",
        "print(\"Smoothed Bigram Probability:\", smoothed_prob)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEUMzxfK5MW1",
        "outputId": "48c2bc4d-aefa-451e-8f3a-c2969fb91f32"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsmoothed Bigram Probability: 0.031746031746031744\n",
            "Smoothed Bigram Probability: 7.72456782099135e-06\n"
          ]
        }
      ]
    }
  ]
}